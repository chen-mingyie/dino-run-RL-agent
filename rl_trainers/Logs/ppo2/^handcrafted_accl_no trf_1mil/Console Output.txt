C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\python.exe "C:\Program Files\JetBrains\PyCharm Community Edition 2020.2.1\plugins\python-ce\helpers\pydev\pydevd.py" --multiproc --qt-support=auto --client 127.0.0.1 --port 63458 --file C:/Users/chen_/PycharmProjects/dino-run_RL-agent/rl_trainers/start_training.py
pydev debugger: process 22784 is connecting

Connected to pydev debugger (build 202.6948.78)
2020-11-20 07:30:00.629606: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-11-20 07:30:00.629920: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\common\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\common\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-11-20 07:30:20.672652: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-11-20 07:30:20.687536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-11-20 07:30:21.708532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.515
pciBusID: 0000:01:00.0
2020-11-20 07:30:21.710410: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-11-20 07:30:21.711886: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_100.dll'; dlerror: cublas64_100.dll not found
2020-11-20 07:30:21.713047: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_100.dll'; dlerror: cufft64_100.dll not found
2020-11-20 07:30:21.714010: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_100.dll'; dlerror: curand64_100.dll not found
2020-11-20 07:30:21.715151: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_100.dll'; dlerror: cusolver64_100.dll not found
2020-11-20 07:30:21.716191: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_100.dll'; dlerror: cusparse64_100.dll not found
2020-11-20 07:30:21.799687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-11-20 07:30:21.799990: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-11-20 07:30:21.929780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-20 07:30:21.930132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-11-20 07:30:21.930354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\common\policies.py:117: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\common\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\common\policies.py:562: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\tensorflow_core\python\layers\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\common\tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\common\distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\common\distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\ppo2\ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\ppo2\ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\tensorflow_core\python\ops\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\ppo2\ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\ppo2\ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

WARNING:tensorflow:From C:\Users\chen_\anaconda3\envs\gym-chrome-dino-master\lib\site-packages\stable_baselines\ppo2\ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

--------------------------------------
| approxkl           | 0.0003805488  |
| clipfrac           | 0.0           |
| ep_len_mean        | 29.8          |
| ep_reward_mean     | -2.71         |
| explained_variance | 0.0512        |
| fps                | 26            |
| n_updates          | 1             |
| policy_entropy     | 0.69269836    |
| policy_loss        | -0.0003231259 |
| serial_timesteps   | 128           |
| time_elapsed       | 0             |
| total_timesteps    | 640           |
| value_loss         | 4.780573      |
--------------------------------------
Num timesteps: 5000
Best mean reward: -inf - Last mean reward per episode: -2.68
Mean score over last 55.00 episodes: 51.96
Saving new best model to Logs\ppo2\best_model
Num timesteps: 10000
Best mean reward: -2.68 - Last mean reward per episode: -4.06
Mean score over last 113.00 episodes: 51.12
Num timesteps: 15000
Best mean reward: -2.68 - Last mean reward per episode: -1.84
Mean score over last 170.00 episodes: 51.43
Saving new best model to Logs\ppo2\best_model
Num timesteps: 20000
Best mean reward: -1.84 - Last mean reward per episode: -3.47
Mean score over last 218.00 episodes: 51.32
Num timesteps: 25000
Best mean reward: -1.84 - Last mean reward per episode: -1.77
Mean score over last 272.00 episodes: 51.10
Saving new best model to Logs\ppo2\best_model
Num timesteps: 30000
Best mean reward: -1.77 - Last mean reward per episode: -2.11
Mean score over last 325.00 episodes: 50.65
--------------------------------------
| approxkl           | 0.00044384148 |
| clipfrac           | 0.0           |
| ep_len_mean        | 35            |
| ep_reward_mean     | -1.51         |
| explained_variance | 0.167         |
| fps                | 32            |
| n_updates          | 50            |
| policy_entropy     | 0.6264312     |
| policy_loss        | -0.0015572439 |
| serial_timesteps   | 6400          |
| time_elapsed       | 1.02e+03      |
| total_timesteps    | 32000         |
| value_loss         | 1.8217908     |
--------------------------------------
Num timesteps: 35000
Best mean reward: -1.77 - Last mean reward per episode: -1.84
Mean score over last 372.00 episodes: 50.42
Num timesteps: 40000
Best mean reward: -1.77 - Last mean reward per episode: -1.28
Mean score over last 424.00 episodes: 50.98
Saving new best model to Logs\ppo2\best_model
Num timesteps: 45000
Best mean reward: -1.28 - Last mean reward per episode: -2.64
Mean score over last 480.00 episodes: 51.14
Num timesteps: 50000
Best mean reward: -1.28 - Last mean reward per episode: -1.58
Mean score over last 529.00 episodes: 51.18
Num timesteps: 55000
Best mean reward: -1.28 - Last mean reward per episode: -0.59
Mean score over last 578.00 episodes: 51.31
Saving new best model to Logs\ppo2\best_model
Num timesteps: 60000
Best mean reward: -0.59 - Last mean reward per episode: 0.55
Mean score over last 622.00 episodes: 51.58
Saving new best model to Logs\ppo2\best_model
--------------------------------------
| approxkl           | 0.00018465976 |
| clipfrac           | 0.000390625   |
| ep_len_mean        | 40.9          |
| ep_reward_mean     | -0.442        |
| explained_variance | 0.221         |
| fps                | 39            |
| n_updates          | 100           |
| policy_entropy     | 0.40539265    |
| policy_loss        | -0.0010344011 |
| serial_timesteps   | 12800         |
| time_elapsed       | 1.99e+03      |
| total_timesteps    | 64000         |
| value_loss         | 2.1406012     |
--------------------------------------
Num timesteps: 65000
Best mean reward: 0.55 - Last mean reward per episode: -0.47
Mean score over last 668.00 episodes: 51.39
Num timesteps: 70000
Best mean reward: 0.55 - Last mean reward per episode: 1.05
Mean score over last 705.00 episodes: 51.57
Saving new best model to Logs\ppo2\best_model
Num timesteps: 75000
Best mean reward: 1.05 - Last mean reward per episode: 1.02
Mean score over last 746.00 episodes: 51.97
Num timesteps: 80000
Best mean reward: 1.05 - Last mean reward per episode: 1.52
Mean score over last 783.00 episodes: 52.22
Saving new best model to Logs\ppo2\best_model
Num timesteps: 85000
Best mean reward: 1.52 - Last mean reward per episode: 1.45
Mean score over last 813.00 episodes: 53.13
Num timesteps: 90000
Best mean reward: 1.52 - Last mean reward per episode: 2.31
Mean score over last 838.00 episodes: 54.30
Saving new best model to Logs\ppo2\best_model
Num timesteps: 95000
Best mean reward: 2.31 - Last mean reward per episode: 3.49
Mean score over last 854.00 episodes: 55.21
Saving new best model to Logs\ppo2\best_model
--------------------------------------
| approxkl           | 0.0015968878  |
| clipfrac           | 0.017578125   |
| ep_len_mean        | 81            |
| ep_reward_mean     | 3.68          |
| explained_variance | 0.293         |
| fps                | 39            |
| n_updates          | 150           |
| policy_entropy     | 0.23007365    |
| policy_loss        | -0.0026403582 |
| serial_timesteps   | 19200         |
| time_elapsed       | 2.83e+03      |
| total_timesteps    | 96000         |
| value_loss         | 0.5828413     |
--------------------------------------
Num timesteps: 100000
Best mean reward: 3.49 - Last mean reward per episode: 3.77
Mean score over last 870.00 episodes: 56.52
Saving new best model to Logs\ppo2\best_model
Num timesteps: 105000
Best mean reward: 3.77 - Last mean reward per episode: 4.00
Mean score over last 892.00 episodes: 57.83
Saving new best model to Logs\ppo2\best_model
Num timesteps: 110000
Best mean reward: 4.00 - Last mean reward per episode: 5.08
Mean score over last 910.00 episodes: 59.60
Saving new best model to Logs\ppo2\best_model
Num timesteps: 115000
Best mean reward: 5.08 - Last mean reward per episode: 5.66
Mean score over last 935.00 episodes: 61.06
Saving new best model to Logs\ppo2\best_model
Num timesteps: 120000
Best mean reward: 5.66 - Last mean reward per episode: 5.52
Mean score over last 958.00 episodes: 62.66
Num timesteps: 125000
Best mean reward: 5.66 - Last mean reward per episode: 5.90
Mean score over last 977.00 episodes: 63.98
Saving new best model to Logs\ppo2\best_model
--------------------------------------
| approxkl           | 0.00093215716 |
| clipfrac           | 0.012109376   |
| ep_len_mean        | 102           |
| ep_reward_mean     | 5.43          |
| explained_variance | 0.1           |
| fps                | 40            |
| n_updates          | 200           |
| policy_entropy     | 0.15280904    |
| policy_loss        | -0.0016417201 |
| serial_timesteps   | 25600         |
| time_elapsed       | 3.66e+03      |
| total_timesteps    | 128000        |
| value_loss         | 3.560758      |
--------------------------------------
Num timesteps: 130000
Best mean reward: 5.90 - Last mean reward per episode: 5.49
Mean score over last 996.00 episodes: 65.60
Num timesteps: 135000
Best mean reward: 5.90 - Last mean reward per episode: 5.79
Mean score over last 1020.00 episodes: 67.17
Num timesteps: 140000
Best mean reward: 5.90 - Last mean reward per episode: 6.51
Mean score over last 1041.00 episodes: 68.65
Saving new best model to Logs\ppo2\best_model
Num timesteps: 145000
Best mean reward: 6.51 - Last mean reward per episode: 6.77
Mean score over last 1056.00 episodes: 69.57
Saving new best model to Logs\ppo2\best_model
Num timesteps: 150000
Best mean reward: 6.77 - Last mean reward per episode: 6.51
Mean score over last 1074.00 episodes: 70.50
Num timesteps: 155000
Best mean reward: 6.77 - Last mean reward per episode: 6.03
Mean score over last 1091.00 episodes: 71.27
Num timesteps: 160000
Best mean reward: 6.77 - Last mean reward per episode: 5.66
Mean score over last 1113.00 episodes: 72.46
--------------------------------------
| approxkl           | 0.0007671583  |
| clipfrac           | 0.00703125    |
| ep_len_mean        | 99.9          |
| ep_reward_mean     | 5.66          |
| explained_variance | 0.212         |
| fps                | 40            |
| n_updates          | 250           |
| policy_entropy     | 0.14358763    |
| policy_loss        | -0.0024698998 |
| serial_timesteps   | 32000         |
| time_elapsed       | 4.48e+03      |
| total_timesteps    | 160000        |
| value_loss         | 1.609925      |
--------------------------------------
Num timesteps: 165000
Best mean reward: 6.77 - Last mean reward per episode: 5.70
Mean score over last 1125.00 episodes: 73.57
Num timesteps: 170000
Best mean reward: 6.77 - Last mean reward per episode: 6.30
Mean score over last 1145.00 episodes: 74.80
Num timesteps: 175000
Best mean reward: 6.77 - Last mean reward per episode: 7.05
Mean score over last 1162.00 episodes: 75.82
Saving new best model to Logs\ppo2\best_model
Num timesteps: 180000
Best mean reward: 7.05 - Last mean reward per episode: 6.55
Mean score over last 1188.00 episodes: 77.09
Num timesteps: 185000
Best mean reward: 7.05 - Last mean reward per episode: 6.02
Mean score over last 1210.00 episodes: 78.04
Num timesteps: 190000
Best mean reward: 7.05 - Last mean reward per episode: 6.55
Mean score over last 1230.00 episodes: 79.38
--------------------------------------
| approxkl           | 0.0011763794  |
| clipfrac           | 0.01328125    |
| ep_len_mean        | 105           |
| ep_reward_mean     | 6.47          |
| explained_variance | 0.185         |
| fps                | 39            |
| n_updates          | 300           |
| policy_entropy     | 0.14176676    |
| policy_loss        | -0.0026664515 |
| serial_timesteps   | 38400         |
| time_elapsed       | 5.31e+03      |
| total_timesteps    | 192000        |
| value_loss         | 1.9043975     |
--------------------------------------
Num timesteps: 195000
Best mean reward: 7.05 - Last mean reward per episode: 6.91
Mean score over last 1254.00 episodes: 80.72
Num timesteps: 200000
Best mean reward: 7.05 - Last mean reward per episode: 6.97
Mean score over last 1270.00 episodes: 81.62
Num timesteps: 205000
Best mean reward: 7.05 - Last mean reward per episode: 6.68
Mean score over last 1287.00 episodes: 82.44
Num timesteps: 210000
Best mean reward: 7.05 - Last mean reward per episode: 6.56
Mean score over last 1302.00 episodes: 83.09
Num timesteps: 215000
Best mean reward: 7.05 - Last mean reward per episode: 6.66
Mean score over last 1318.00 episodes: 83.93
Num timesteps: 220000
Best mean reward: 7.05 - Last mean reward per episode: 6.72
Mean score over last 1334.00 episodes: 84.91
--------------------------------------
| approxkl           | 0.00045205627 |
| clipfrac           | 0.0054687504  |
| ep_len_mean        | 119           |
| ep_reward_mean     | 6.8           |
| explained_variance | 0.0135        |
| fps                | 40            |
| n_updates          | 350           |
| policy_entropy     | 0.13274476    |
| policy_loss        | -0.0023455252 |
| serial_timesteps   | 44800         |
| time_elapsed       | 6.11e+03      |
| total_timesteps    | 224000        |
| value_loss         | 3.0872967     |
--------------------------------------
Num timesteps: 225000
Best mean reward: 7.05 - Last mean reward per episode: 7.21
Mean score over last 1349.00 episodes: 85.57
Saving new best model to Logs\ppo2\best_model
Num timesteps: 230000
Best mean reward: 7.21 - Last mean reward per episode: 7.90
Mean score over last 1363.00 episodes: 86.26
Saving new best model to Logs\ppo2\best_model
Num timesteps: 235000
Best mean reward: 7.90 - Last mean reward per episode: 6.86
Mean score over last 1381.00 episodes: 86.98
Num timesteps: 240000
Best mean reward: 7.90 - Last mean reward per episode: 6.45
Mean score over last 1399.00 episodes: 87.87
Num timesteps: 245000
Best mean reward: 7.90 - Last mean reward per episode: 6.85
Mean score over last 1413.00 episodes: 88.69
Num timesteps: 250000
Best mean reward: 7.90 - Last mean reward per episode: 6.57
Mean score over last 1424.00 episodes: 89.12
Num timesteps: 255000
Best mean reward: 7.90 - Last mean reward per episode: 6.17
Mean score over last 1444.00 episodes: 89.90
--------------------------------------
| approxkl           | 0.0008829867  |
| clipfrac           | 0.00859375    |
| ep_len_mean        | 109           |
| ep_reward_mean     | 5.76          |
| explained_variance | 0.181         |
| fps                | 38            |
| n_updates          | 400           |
| policy_entropy     | 0.12247795    |
| policy_loss        | -0.0049220934 |
| serial_timesteps   | 51200         |
| time_elapsed       | 6.93e+03      |
| total_timesteps    | 256000        |
| value_loss         | 2.5981412     |
--------------------------------------
Num timesteps: 260000
Best mean reward: 7.90 - Last mean reward per episode: 6.53
Mean score over last 1453.00 episodes: 90.20
Num timesteps: 265000
Best mean reward: 7.90 - Last mean reward per episode: 6.73
Mean score over last 1467.00 episodes: 90.91
Num timesteps: 270000
Best mean reward: 7.90 - Last mean reward per episode: 7.17
Mean score over last 1479.00 episodes: 91.63
Num timesteps: 275000
Best mean reward: 7.90 - Last mean reward per episode: 7.23
Mean score over last 1490.00 episodes: 92.46
Num timesteps: 280000
Best mean reward: 7.90 - Last mean reward per episode: 6.67
Mean score over last 1507.00 episodes: 93.03
Num timesteps: 285000
Best mean reward: 7.90 - Last mean reward per episode: 6.88
Mean score over last 1525.00 episodes: 93.85
--------------------------------------
| approxkl           | 0.00077292253 |
| clipfrac           | 0.00859375    |
| ep_len_mean        | 126           |
| ep_reward_mean     | 7.49          |
| explained_variance | 0.368         |
| fps                | 39            |
| n_updates          | 450           |
| policy_entropy     | 0.12656026    |
| policy_loss        | -0.0020493832 |
| serial_timesteps   | 57600         |
| time_elapsed       | 7.74e+03      |
| total_timesteps    | 288000        |
| value_loss         | 0.66816866    |
--------------------------------------
Num timesteps: 290000
Best mean reward: 7.90 - Last mean reward per episode: 7.55
Mean score over last 1543.00 episodes: 94.78
Num timesteps: 295000
Best mean reward: 7.90 - Last mean reward per episode: 8.07
Mean score over last 1558.00 episodes: 95.34
Saving new best model to Logs\ppo2\best_model
Num timesteps: 300000
Best mean reward: 8.07 - Last mean reward per episode: 8.26
Mean score over last 1575.00 episodes: 96.13
Saving new best model to Logs\ppo2\best_model
Num timesteps: 305000
Best mean reward: 8.26 - Last mean reward per episode: 7.83
Mean score over last 1590.00 episodes: 96.71
Num timesteps: 310000
Best mean reward: 8.26 - Last mean reward per episode: 7.20
Mean score over last 1606.00 episodes: 97.37
Num timesteps: 315000
Best mean reward: 8.26 - Last mean reward per episode: 7.12
Mean score over last 1621.00 episodes: 97.80
Num timesteps: 320000
Best mean reward: 8.26 - Last mean reward per episode: 6.88
Mean score over last 1632.00 episodes: 98.28
--------------------------------------
| approxkl           | 0.0013213743  |
| clipfrac           | 0.0125        |
| ep_len_mean        | 114           |
| ep_reward_mean     | 6.88          |
| explained_variance | 0.167         |
| fps                | 38            |
| n_updates          | 500           |
| policy_entropy     | 0.13480979    |
| policy_loss        | -0.0028113571 |
| serial_timesteps   | 64000         |
| time_elapsed       | 8.55e+03      |
| total_timesteps    | 320000        |
| value_loss         | 2.989196      |
--------------------------------------
Num timesteps: 325000
Best mean reward: 8.26 - Last mean reward per episode: 6.70
Mean score over last 1651.00 episodes: 98.89
Num timesteps: 330000
Best mean reward: 8.26 - Last mean reward per episode: 6.49
Mean score over last 1677.00 episodes: 99.44
Num timesteps: 335000
Best mean reward: 8.26 - Last mean reward per episode: 7.10
Mean score over last 1695.00 episodes: 100.10
Num timesteps: 340000
Best mean reward: 8.26 - Last mean reward per episode: 8.12
Mean score over last 1713.00 episodes: 100.71
Num timesteps: 345000
Best mean reward: 8.26 - Last mean reward per episode: 8.28
Mean score over last 1729.00 episodes: 101.46
Saving new best model to Logs\ppo2\best_model
Num timesteps: 350000
Best mean reward: 8.28 - Last mean reward per episode: 7.20
Mean score over last 1743.00 episodes: 101.80
--------------------------------------
| approxkl           | 0.0007445847  |
| clipfrac           | 0.010546876   |
| ep_len_mean        | 116           |
| ep_reward_mean     | 7.11          |
| explained_variance | 0.0452        |
| fps                | 37            |
| n_updates          | 550           |
| policy_entropy     | 0.11263009    |
| policy_loss        | -0.0011599407 |
| serial_timesteps   | 70400         |
| time_elapsed       | 9.4e+03       |
| total_timesteps    | 352000        |
| value_loss         | 1.900465      |
--------------------------------------
Num timesteps: 355000
Best mean reward: 8.28 - Last mean reward per episode: 7.18
Mean score over last 1759.00 episodes: 102.65
Num timesteps: 360000
Best mean reward: 8.28 - Last mean reward per episode: 7.77
Mean score over last 1781.00 episodes: 103.59
Num timesteps: 365000
Best mean reward: 8.28 - Last mean reward per episode: 8.09
Mean score over last 1797.00 episodes: 104.08
Num timesteps: 370000
Best mean reward: 8.28 - Last mean reward per episode: 8.55
Mean score over last 1816.00 episodes: 104.68
Saving new best model to Logs\ppo2\best_model
Num timesteps: 375000
Best mean reward: 8.55 - Last mean reward per episode: 8.56
Mean score over last 1832.00 episodes: 105.53
Saving new best model to Logs\ppo2\best_model
Num timesteps: 380000
Best mean reward: 8.56 - Last mean reward per episode: 8.03
Mean score over last 1843.00 episodes: 106.00
---------------------------------------
| approxkl           | 0.0011836232   |
| clipfrac           | 0.010546875    |
| ep_len_mean        | 133            |
| ep_reward_mean     | 8.48           |
| explained_variance | 0.348          |
| fps                | 39             |
| n_updates          | 600            |
| policy_entropy     | 0.13081692     |
| policy_loss        | -0.00011737867 |
| serial_timesteps   | 76800          |
| time_elapsed       | 1.02e+04       |
| total_timesteps    | 384000         |
| value_loss         | 0.6582362      |
---------------------------------------
Num timesteps: 385000
Best mean reward: 8.56 - Last mean reward per episode: 8.34
Mean score over last 1855.00 episodes: 106.46
Num timesteps: 390000
Best mean reward: 8.56 - Last mean reward per episode: 8.58
Mean score over last 1869.00 episodes: 107.34
Saving new best model to Logs\ppo2\best_model
Num timesteps: 395000
Best mean reward: 8.58 - Last mean reward per episode: 9.32
Mean score over last 1886.00 episodes: 108.08
Saving new best model to Logs\ppo2\best_model
Num timesteps: 400000
Best mean reward: 9.32 - Last mean reward per episode: 10.09
Mean score over last 1899.00 episodes: 108.83
Saving new best model to Logs\ppo2\best_model
Num timesteps: 405000
Best mean reward: 10.09 - Last mean reward per episode: 8.92
Mean score over last 1915.00 episodes: 109.26
Num timesteps: 410000
Best mean reward: 10.09 - Last mean reward per episode: 8.17
Mean score over last 1928.00 episodes: 109.69
Num timesteps: 415000
Best mean reward: 10.09 - Last mean reward per episode: 6.68
Mean score over last 1943.00 episodes: 110.00
--------------------------------------
| approxkl           | 0.00084148097 |
| clipfrac           | 0.011328126   |
| ep_len_mean        | 121           |
| ep_reward_mean     | 7             |
| explained_variance | 0.15          |
| fps                | 37            |
| n_updates          | 650           |
| policy_entropy     | 0.11414985    |
| policy_loss        | -0.004592573  |
| serial_timesteps   | 83200         |
| time_elapsed       | 1.11e+04      |
| total_timesteps    | 416000        |
| value_loss         | 1.9285678     |
--------------------------------------
Num timesteps: 420000
Best mean reward: 10.09 - Last mean reward per episode: 7.33
Mean score over last 1958.00 episodes: 110.46
Num timesteps: 425000
Best mean reward: 10.09 - Last mean reward per episode: 5.60
Mean score over last 1977.00 episodes: 110.69
Num timesteps: 430000
Best mean reward: 10.09 - Last mean reward per episode: 5.09
Mean score over last 1991.00 episodes: 110.85
Num timesteps: 435000
Best mean reward: 10.09 - Last mean reward per episode: 6.12
Mean score over last 2003.00 episodes: 111.32
Num timesteps: 440000
Best mean reward: 10.09 - Last mean reward per episode: 7.15
Mean score over last 2017.00 episodes: 111.87
Num timesteps: 445000
Best mean reward: 10.09 - Last mean reward per episode: 6.97
Mean score over last 2031.00 episodes: 112.05
--------------------------------------
| approxkl           | 0.0008290937  |
| clipfrac           | 0.00859375    |
| ep_len_mean        | 131           |
| ep_reward_mean     | 6.73          |
| explained_variance | 0.13          |
| fps                | 38            |
| n_updates          | 700           |
| policy_entropy     | 0.10024293    |
| policy_loss        | -0.0016871404 |
| serial_timesteps   | 89600         |
| time_elapsed       | 1.19e+04      |
| total_timesteps    | 448000        |
| value_loss         | 5.960146      |
--------------------------------------
Num timesteps: 450000
Best mean reward: 10.09 - Last mean reward per episode: 6.70
Mean score over last 2042.00 episodes: 112.63
Num timesteps: 455000
Best mean reward: 10.09 - Last mean reward per episode: 7.36
Mean score over last 2060.00 episodes: 113.06
Num timesteps: 460000
Best mean reward: 10.09 - Last mean reward per episode: 7.93
Mean score over last 2079.00 episodes: 113.38
Num timesteps: 465000
Best mean reward: 10.09 - Last mean reward per episode: 8.10
Mean score over last 2089.00 episodes: 113.90
Num timesteps: 470000
Best mean reward: 10.09 - Last mean reward per episode: 8.35
Mean score over last 2100.00 episodes: 114.15
Num timesteps: 475000
Best mean reward: 10.09 - Last mean reward per episode: 9.40
Mean score over last 2118.00 episodes: 114.72
Num timesteps: 480000
Best mean reward: 10.09 - Last mean reward per episode: 9.49
Mean score over last 2132.00 episodes: 115.19
--------------------------------------
| approxkl           | 0.00097397424 |
| clipfrac           | 0.010937501   |
| ep_len_mean        | 140           |
| ep_reward_mean     | 9.49          |
| explained_variance | 0.211         |
| fps                | 40            |
| n_updates          | 750           |
| policy_entropy     | 0.12339994    |
| policy_loss        | -0.0024437753 |
| serial_timesteps   | 96000         |
| time_elapsed       | 1.27e+04      |
| total_timesteps    | 480000        |
| value_loss         | 2.9835706     |
--------------------------------------
Num timesteps: 485000
Best mean reward: 10.09 - Last mean reward per episode: 8.94
Mean score over last 2140.00 episodes: 115.57
Num timesteps: 490000
Best mean reward: 10.09 - Last mean reward per episode: 9.66
Mean score over last 2151.00 episodes: 116.45
Num timesteps: 495000
Best mean reward: 10.09 - Last mean reward per episode: 9.43
Mean score over last 2163.00 episodes: 117.09
Num timesteps: 500000
Best mean reward: 10.09 - Last mean reward per episode: 10.22
Mean score over last 2172.00 episodes: 117.76
Saving new best model to Logs\ppo2\best_model
Num timesteps: 505000
Best mean reward: 10.22 - Last mean reward per episode: 9.13
Mean score over last 2190.00 episodes: 118.42
Num timesteps: 510000
Best mean reward: 10.22 - Last mean reward per episode: 8.70
Mean score over last 2208.00 episodes: 119.00
--------------------------------------
| approxkl           | 0.00034530062 |
| clipfrac           | 0.00390625    |
| ep_len_mean        | 128           |
| ep_reward_mean     | 8.42          |
| explained_variance | 0.118         |
| fps                | 34            |
| n_updates          | 800           |
| policy_entropy     | 0.12715188    |
| policy_loss        | -0.0016879379 |
| serial_timesteps   | 102400        |
| time_elapsed       | 1.36e+04      |
| total_timesteps    | 512000        |
| value_loss         | 7.101915      |
--------------------------------------
Num timesteps: 515000
Best mean reward: 10.22 - Last mean reward per episode: 7.38
Mean score over last 2229.00 episodes: 119.14
Num timesteps: 520000
Best mean reward: 10.22 - Last mean reward per episode: 6.93
Mean score over last 2241.00 episodes: 119.68
Num timesteps: 525000
Best mean reward: 10.22 - Last mean reward per episode: 6.29
Mean score over last 2253.00 episodes: 119.98
Num timesteps: 530000
Best mean reward: 10.22 - Last mean reward per episode: 6.62
Mean score over last 2266.00 episodes: 120.69
Num timesteps: 535000
Best mean reward: 10.22 - Last mean reward per episode: 6.04
Mean score over last 2280.00 episodes: 121.20
Num timesteps: 540000
Best mean reward: 10.22 - Last mean reward per episode: 5.43
Mean score over last 2298.00 episodes: 121.33
--------------------------------------
| approxkl           | 0.00072374794 |
| clipfrac           | 0.0070312503  |
| ep_len_mean        | 111           |
| ep_reward_mean     | 6.09          |
| explained_variance | 0.249         |
| fps                | 34            |
| n_updates          | 850           |
| policy_entropy     | 0.13761377    |
| policy_loss        | -0.0009763526 |
| serial_timesteps   | 108800        |
| time_elapsed       | 1.45e+04      |
| total_timesteps    | 544000        |
| value_loss         | 1.6239465     |
--------------------------------------
Num timesteps: 545000
Best mean reward: 10.22 - Last mean reward per episode: 6.07
Mean score over last 2317.00 episodes: 122.04
Num timesteps: 550000
Best mean reward: 10.22 - Last mean reward per episode: 6.25
Mean score over last 2330.00 episodes: 122.38
Num timesteps: 555000
Best mean reward: 10.22 - Last mean reward per episode: 7.16
Mean score over last 2344.00 episodes: 122.86
Num timesteps: 560000
Best mean reward: 10.22 - Last mean reward per episode: 7.91
Mean score over last 2362.00 episodes: 123.39
Num timesteps: 565000
Best mean reward: 10.22 - Last mean reward per episode: 7.85
Mean score over last 2378.00 episodes: 123.62
Num timesteps: 570000
Best mean reward: 10.22 - Last mean reward per episode: 8.11
Mean score over last 2397.00 episodes: 124.26
Num timesteps: 575000
Best mean reward: 10.22 - Last mean reward per episode: 8.98
Mean score over last 2413.00 episodes: 124.57
-------------------------------------
| approxkl           | 0.0038438356 |
| clipfrac           | 0.022265622  |
| ep_len_mean        | 130          |
| ep_reward_mean     | 9.07         |
| explained_variance | 0.141        |
| fps                | 36           |
| n_updates          | 900          |
| policy_entropy     | 0.14049055   |
| policy_loss        | -0.005062042 |
| serial_timesteps   | 115200       |
| time_elapsed       | 1.53e+04     |
| total_timesteps    | 576000       |
| value_loss         | 1.746011     |
-------------------------------------
Num timesteps: 580000
Best mean reward: 10.22 - Last mean reward per episode: 9.26
Mean score over last 2431.00 episodes: 124.90
Num timesteps: 585000
Best mean reward: 10.22 - Last mean reward per episode: 8.74
Mean score over last 2452.00 episodes: 125.46
Num timesteps: 590000
Best mean reward: 10.22 - Last mean reward per episode: 8.11
Mean score over last 2475.00 episodes: 125.69
Num timesteps: 595000
Best mean reward: 10.22 - Last mean reward per episode: 9.01
Mean score over last 2492.00 episodes: 126.36
Num timesteps: 600000
Best mean reward: 10.22 - Last mean reward per episode: 9.22
Mean score over last 2507.00 episodes: 126.61
Num timesteps: 605000
Best mean reward: 10.22 - Last mean reward per episode: 8.76
Mean score over last 2517.00 episodes: 126.97
--------------------------------------
| approxkl           | 0.0026110657  |
| clipfrac           | 0.026171874   |
| ep_len_mean        | 128           |
| ep_reward_mean     | 7.82          |
| explained_variance | 0.276         |
| fps                | 34            |
| n_updates          | 950           |
| policy_entropy     | 0.10670681    |
| policy_loss        | -0.0027319596 |
| serial_timesteps   | 121600        |
| time_elapsed       | 1.62e+04      |
| total_timesteps    | 608000        |
| value_loss         | 0.71846104    |
--------------------------------------
Num timesteps: 610000
Best mean reward: 10.22 - Last mean reward per episode: 7.74
Mean score over last 2534.00 episodes: 127.20
Num timesteps: 615000
Best mean reward: 10.22 - Last mean reward per episode: 8.27
Mean score over last 2545.00 episodes: 127.84
Num timesteps: 620000
Best mean reward: 10.22 - Last mean reward per episode: 8.64
Mean score over last 2563.00 episodes: 128.26
Num timesteps: 625000
Best mean reward: 10.22 - Last mean reward per episode: 8.96
Mean score over last 2578.00 episodes: 128.72
Num timesteps: 630000
Best mean reward: 10.22 - Last mean reward per episode: 8.90
Mean score over last 2594.00 episodes: 129.20
Num timesteps: 635000
Best mean reward: 10.22 - Last mean reward per episode: 8.84
Mean score over last 2610.00 episodes: 129.80
Num timesteps: 640000
Best mean reward: 10.22 - Last mean reward per episode: 8.43
Mean score over last 2622.00 episodes: 130.13
--------------------------------------
| approxkl           | 0.0011690373  |
| clipfrac           | 0.011328124   |
| ep_len_mean        | 134           |
| ep_reward_mean     | 8.43          |
| explained_variance | 0.00951       |
| fps                | 36            |
| n_updates          | 1000          |
| policy_entropy     | 0.119641654   |
| policy_loss        | -0.0018041742 |
| serial_timesteps   | 128000        |
| time_elapsed       | 1.71e+04      |
| total_timesteps    | 640000        |
| value_loss         | 0.7422613     |
--------------------------------------
Num timesteps: 645000
Best mean reward: 10.22 - Last mean reward per episode: 8.94
Mean score over last 2641.00 episodes: 130.80
Num timesteps: 650000
Best mean reward: 10.22 - Last mean reward per episode: 8.06
Mean score over last 2654.00 episodes: 130.91
Num timesteps: 655000
Best mean reward: 10.22 - Last mean reward per episode: 8.99
Mean score over last 2666.00 episodes: 131.01
Num timesteps: 660000
Best mean reward: 10.22 - Last mean reward per episode: 8.53
Mean score over last 2685.00 episodes: 131.49
Num timesteps: 665000
Best mean reward: 10.22 - Last mean reward per episode: 8.76
Mean score over last 2699.00 episodes: 131.73
Num timesteps: 670000
Best mean reward: 10.22 - Last mean reward per episode: 8.81
Mean score over last 2712.00 episodes: 132.31
-------------------------------------
| approxkl           | 0.0009175885 |
| clipfrac           | 0.01328125   |
| ep_len_mean        | 143          |
| ep_reward_mean     | 9.5          |
| explained_variance | 0.16         |
| fps                | 39           |
| n_updates          | 1050         |
| policy_entropy     | 0.10712142   |
| policy_loss        | -0.002779931 |
| serial_timesteps   | 134400       |
| time_elapsed       | 1.8e+04      |
| total_timesteps    | 672000       |
| value_loss         | 0.70044667   |
-------------------------------------
Num timesteps: 675000
Best mean reward: 10.22 - Last mean reward per episode: 9.59
Mean score over last 2723.00 episodes: 132.63
Num timesteps: 680000
Best mean reward: 10.22 - Last mean reward per episode: 9.83
Mean score over last 2737.00 episodes: 132.94
Num timesteps: 685000
Best mean reward: 10.22 - Last mean reward per episode: 9.31
Mean score over last 2751.00 episodes: 133.64
Num timesteps: 690000
Best mean reward: 10.22 - Last mean reward per episode: 9.09
Mean score over last 2759.00 episodes: 133.93
Num timesteps: 695000
Best mean reward: 10.22 - Last mean reward per episode: 9.19
Mean score over last 2779.00 episodes: 134.10
Num timesteps: 700000
Best mean reward: 10.22 - Last mean reward per episode: 8.91
Mean score over last 2794.00 episodes: 134.54
--------------------------------------
| approxkl           | 0.0012410929  |
| clipfrac           | 0.009765625   |
| ep_len_mean        | 148           |
| ep_reward_mean     | 9.84          |
| explained_variance | 0.0703        |
| fps                | 38            |
| n_updates          | 1100          |
| policy_entropy     | 0.14095066    |
| policy_loss        | -0.0025521873 |
| serial_timesteps   | 140800        |
| time_elapsed       | 1.88e+04      |
| total_timesteps    | 704000        |
| value_loss         | 4.0700274     |
--------------------------------------
Num timesteps: 705000
Best mean reward: 10.22 - Last mean reward per episode: 9.62
Mean score over last 2806.00 episodes: 135.02
Num timesteps: 710000
Best mean reward: 10.22 - Last mean reward per episode: 10.40
Mean score over last 2818.00 episodes: 135.34
Saving new best model to Logs\ppo2\best_model
Num timesteps: 715000
Best mean reward: 10.40 - Last mean reward per episode: 10.61
Mean score over last 2834.00 episodes: 135.70
Saving new best model to Logs\ppo2\best_model
Num timesteps: 720000
Best mean reward: 10.61 - Last mean reward per episode: 10.85
Mean score over last 2851.00 episodes: 135.95
Saving new best model to Logs\ppo2\best_model
Num timesteps: 725000
Best mean reward: 10.85 - Last mean reward per episode: 8.57
Mean score over last 2871.00 episodes: 135.89
Num timesteps: 730000
Best mean reward: 10.85 - Last mean reward per episode: 7.79
Mean score over last 2890.00 episodes: 136.32
Num timesteps: 735000
Best mean reward: 10.85 - Last mean reward per episode: 7.39
Mean score over last 2902.00 episodes: 136.60
--------------------------------------
| approxkl           | 0.0005045825  |
| clipfrac           | 0.005859375   |
| ep_len_mean        | 131           |
| ep_reward_mean     | 7.45          |
| explained_variance | 0.168         |
| fps                | 34            |
| n_updates          | 1150          |
| policy_entropy     | 0.13449295    |
| policy_loss        | -0.0006594911 |
| serial_timesteps   | 147200        |
| time_elapsed       | 1.97e+04      |
| total_timesteps    | 736000        |
| value_loss         | 4.806973      |
--------------------------------------
Num timesteps: 740000
Best mean reward: 10.85 - Last mean reward per episode: 8.46
Mean score over last 2913.00 episodes: 137.04
Num timesteps: 745000
Best mean reward: 10.85 - Last mean reward per episode: 8.94
Mean score over last 2926.00 episodes: 137.59
Num timesteps: 750000
Best mean reward: 10.85 - Last mean reward per episode: 8.88
Mean score over last 2938.00 episodes: 138.00
Num timesteps: 755000
Best mean reward: 10.85 - Last mean reward per episode: 9.69
Mean score over last 2953.00 episodes: 138.70
Num timesteps: 760000
Best mean reward: 10.85 - Last mean reward per episode: 9.96
Mean score over last 2966.00 episodes: 138.99
Num timesteps: 765000
Best mean reward: 10.85 - Last mean reward per episode: 9.87
Mean score over last 2982.00 episodes: 139.43
--------------------------------------
| approxkl           | 0.0008853442  |
| clipfrac           | 0.010937501   |
| ep_len_mean        | 138           |
| ep_reward_mean     | 9.52          |
| explained_variance | 0.165         |
| fps                | 36            |
| n_updates          | 1200          |
| policy_entropy     | 0.09867103    |
| policy_loss        | -0.0011551328 |
| serial_timesteps   | 153600        |
| time_elapsed       | 2.06e+04      |
| total_timesteps    | 768000        |
| value_loss         | 2.0038557     |
--------------------------------------
Num timesteps: 770000
Best mean reward: 10.85 - Last mean reward per episode: 9.91
Mean score over last 2996.00 episodes: 139.86
Num timesteps: 775000
Best mean reward: 10.85 - Last mean reward per episode: 10.07
Mean score over last 3012.00 episodes: 140.44
Num timesteps: 780000
Best mean reward: 10.85 - Last mean reward per episode: 10.20
Mean score over last 3028.00 episodes: 141.02
Num timesteps: 785000
Best mean reward: 10.85 - Last mean reward per episode: 9.55
Mean score over last 3040.00 episodes: 141.22
Num timesteps: 790000
Best mean reward: 10.85 - Last mean reward per episode: 9.87
Mean score over last 3051.00 episodes: 141.61
Num timesteps: 795000
Best mean reward: 10.85 - Last mean reward per episode: 10.89
Mean score over last 3063.00 episodes: 142.17
Saving new best model to Logs\ppo2\best_model
Num timesteps: 800000
Best mean reward: 10.89 - Last mean reward per episode: 11.61
Mean score over last 3075.00 episodes: 142.43
Saving new best model to Logs\ppo2\best_model
-------------------------------------
| approxkl           | 0.0012365703 |
| clipfrac           | 0.012890625  |
| ep_len_mean        | 156          |
| ep_reward_mean     | 11.6         |
| explained_variance | -0.00414     |
| fps                | 36           |
| n_updates          | 1250         |
| policy_entropy     | 0.14569008   |
| policy_loss        | -0.002125527 |
| serial_timesteps   | 160000       |
| time_elapsed       | 2.15e+04     |
| total_timesteps    | 800000       |
| value_loss         | 0.99919057   |
-------------------------------------
Num timesteps: 805000
Best mean reward: 11.61 - Last mean reward per episode: 11.78
Mean score over last 3094.00 episodes: 143.03
Saving new best model to Logs\ppo2\best_model
Num timesteps: 810000
Best mean reward: 11.78 - Last mean reward per episode: 10.71
Mean score over last 3106.00 episodes: 143.61
Num timesteps: 815000
Best mean reward: 11.78 - Last mean reward per episode: 10.27
Mean score over last 3122.00 episodes: 143.96
Num timesteps: 820000
Best mean reward: 11.78 - Last mean reward per episode: 11.08
Mean score over last 3134.00 episodes: 144.25
Num timesteps: 825000
Best mean reward: 11.78 - Last mean reward per episode: 11.24
Mean score over last 3150.00 episodes: 144.77
Num timesteps: 830000
Best mean reward: 11.78 - Last mean reward per episode: 11.11
Mean score over last 3163.00 episodes: 145.01
--------------------------------------
| approxkl           | 0.001015085   |
| clipfrac           | 0.008593751   |
| ep_len_mean        | 160           |
| ep_reward_mean     | 10.9          |
| explained_variance | 0.167         |
| fps                | 36            |
| n_updates          | 1300          |
| policy_entropy     | 0.15374573    |
| policy_loss        | -0.0019888259 |
| serial_timesteps   | 166400        |
| time_elapsed       | 2.24e+04      |
| total_timesteps    | 832000        |
| value_loss         | 4.5598464     |
--------------------------------------
Num timesteps: 835000
Best mean reward: 11.78 - Last mean reward per episode: 10.90
Mean score over last 3174.00 episodes: 145.31
Num timesteps: 840000
Best mean reward: 11.78 - Last mean reward per episode: 11.50
Mean score over last 3184.00 episodes: 145.56
Num timesteps: 845000
Best mean reward: 11.78 - Last mean reward per episode: 11.40
Mean score over last 3198.00 episodes: 146.21
Num timesteps: 850000
Best mean reward: 11.78 - Last mean reward per episode: 11.21
Mean score over last 3208.00 episodes: 146.55
Num timesteps: 855000
Best mean reward: 11.78 - Last mean reward per episode: 10.68
Mean score over last 3221.00 episodes: 147.19
Num timesteps: 860000
Best mean reward: 11.78 - Last mean reward per episode: 10.64
Mean score over last 3236.00 episodes: 147.73
--------------------------------------
| approxkl           | 0.0011920849  |
| clipfrac           | 0.008203125   |
| ep_len_mean        | 170           |
| ep_reward_mean     | 11.7          |
| explained_variance | 0.189         |
| fps                | 35            |
| n_updates          | 1350          |
| policy_entropy     | 0.14220378    |
| policy_loss        | -0.0019755184 |
| serial_timesteps   | 172800        |
| time_elapsed       | 2.33e+04      |
| total_timesteps    | 864000        |
| value_loss         | 3.4114578     |
--------------------------------------
Num timesteps: 865000
Best mean reward: 11.78 - Last mean reward per episode: 11.37
Mean score over last 3243.00 episodes: 147.89
Num timesteps: 870000
Best mean reward: 11.78 - Last mean reward per episode: 12.00
Mean score over last 3259.00 episodes: 148.17
Saving new best model to Logs\ppo2\best_model
Num timesteps: 875000
Best mean reward: 12.00 - Last mean reward per episode: 11.53
Mean score over last 3271.00 episodes: 148.54
Num timesteps: 880000
Best mean reward: 12.00 - Last mean reward per episode: 11.26
Mean score over last 3285.00 episodes: 148.70
Num timesteps: 885000
Best mean reward: 12.00 - Last mean reward per episode: 10.08
Mean score over last 3299.00 episodes: 149.05
Num timesteps: 890000
Best mean reward: 12.00 - Last mean reward per episode: 11.13
Mean score over last 3316.00 episodes: 150.03
Num timesteps: 895000
Best mean reward: 12.00 - Last mean reward per episode: 11.77
Mean score over last 3328.00 episodes: 150.28
--------------------------------------
| approxkl           | 0.0019736015  |
| clipfrac           | 0.024609376   |
| ep_len_mean        | 169           |
| ep_reward_mean     | 11.9          |
| explained_variance | 0.156         |
| fps                | 36            |
| n_updates          | 1400          |
| policy_entropy     | 0.1498861     |
| policy_loss        | -0.0026514921 |
| serial_timesteps   | 179200        |
| time_elapsed       | 2.41e+04      |
| total_timesteps    | 896000        |
| value_loss         | 2.151401      |
--------------------------------------
Num timesteps: 900000
Best mean reward: 12.00 - Last mean reward per episode: 12.07
Mean score over last 3340.00 episodes: 150.22
Saving new best model to Logs\ppo2\best_model
Num timesteps: 905000
Best mean reward: 12.07 - Last mean reward per episode: 12.22
Mean score over last 3355.00 episodes: 150.77
Saving new best model to Logs\ppo2\best_model
Num timesteps: 910000
Best mean reward: 12.22 - Last mean reward per episode: 11.41
Mean score over last 3371.00 episodes: 151.03
Num timesteps: 915000
Best mean reward: 12.22 - Last mean reward per episode: 12.35
Mean score over last 3383.00 episodes: 151.33
Saving new best model to Logs\ppo2\best_model
Num timesteps: 920000
Best mean reward: 12.35 - Last mean reward per episode: 12.91
Mean score over last 3394.00 episodes: 151.82
Saving new best model to Logs\ppo2\best_model
Num timesteps: 925000
Best mean reward: 12.91 - Last mean reward per episode: 11.91
Mean score over last 3402.00 episodes: 152.24
--------------------------------------
| approxkl           | 0.004143959   |
| clipfrac           | 0.024609374   |
| ep_len_mean        | 176           |
| ep_reward_mean     | 12.4          |
| explained_variance | 0.0136        |
| fps                | 35            |
| n_updates          | 1450          |
| policy_entropy     | 0.14978017    |
| policy_loss        | -0.0049523767 |
| serial_timesteps   | 185600        |
| time_elapsed       | 2.5e+04       |
| total_timesteps    | 928000        |
| value_loss         | 3.8099742     |
--------------------------------------
Num timesteps: 930000
Best mean reward: 12.91 - Last mean reward per episode: 13.13
Mean score over last 3411.00 episodes: 152.56
Saving new best model to Logs\ppo2\best_model
Num timesteps: 935000
Best mean reward: 13.13 - Last mean reward per episode: 13.17
Mean score over last 3419.00 episodes: 152.95
Saving new best model to Logs\ppo2\best_model
Num timesteps: 940000
Best mean reward: 13.17 - Last mean reward per episode: 13.17
Mean score over last 3435.00 episodes: 153.04
Saving new best model to Logs\ppo2\best_model
Num timesteps: 945000
Best mean reward: 13.17 - Last mean reward per episode: 11.99
Mean score over last 3453.00 episodes: 153.50
Num timesteps: 950000
Best mean reward: 13.17 - Last mean reward per episode: 10.36
Mean score over last 3462.00 episodes: 153.89
Num timesteps: 955000
Best mean reward: 13.17 - Last mean reward per episode: 9.58
Mean score over last 3471.00 episodes: 153.82
Num timesteps: 960000
Best mean reward: 13.17 - Last mean reward per episode: 10.40
Mean score over last 3481.00 episodes: 153.91
--------------------------------------
| approxkl           | 0.0018318315  |
| clipfrac           | 0.013671875   |
| ep_len_mean        | 154           |
| ep_reward_mean     | 10.4          |
| explained_variance | 0.202         |
| fps                | 36            |
| n_updates          | 1500          |
| policy_entropy     | 0.11031788    |
| policy_loss        | -0.0013624188 |
| serial_timesteps   | 192000        |
| time_elapsed       | 2.59e+04      |
| total_timesteps    | 960000        |
| value_loss         | 2.29132       |
--------------------------------------
Num timesteps: 965000
Best mean reward: 13.17 - Last mean reward per episode: 11.21
Mean score over last 3494.00 episodes: 154.43
Num timesteps: 970000
Best mean reward: 13.17 - Last mean reward per episode: 11.72
Mean score over last 3505.00 episodes: 154.66
Num timesteps: 975000
Best mean reward: 13.17 - Last mean reward per episode: 13.34
Mean score over last 3515.00 episodes: 155.67
Saving new best model to Logs\ppo2\best_model
Num timesteps: 980000
Best mean reward: 13.34 - Last mean reward per episode: 14.40
Mean score over last 3524.00 episodes: 155.79
Saving new best model to Logs\ppo2\best_model
Num timesteps: 985000
Best mean reward: 14.40 - Last mean reward per episode: 15.05
Mean score over last 3531.00 episodes: 156.17
Saving new best model to Logs\ppo2\best_model
Num timesteps: 990000
Best mean reward: 15.05 - Last mean reward per episode: 16.09
Mean score over last 3540.00 episodes: 156.94
Saving new best model to Logs\ppo2\best_model
--------------------------------------
| approxkl           | 0.0023282163  |
| clipfrac           | 0.017578125   |
| ep_len_mean        | 195           |
| ep_reward_mean     | 14.6          |
| explained_variance | 0.0587        |
| fps                | 36            |
| n_updates          | 1550          |
| policy_entropy     | 0.14248261    |
| policy_loss        | -0.0017979925 |
| serial_timesteps   | 198400        |
| time_elapsed       | 2.68e+04      |
| total_timesteps    | 992000        |
| value_loss         | 2.2034187     |
--------------------------------------
Num timesteps: 995000
Best mean reward: 16.09 - Last mean reward per episode: 14.23
Mean score over last 3551.00 episodes: 157.09

Process finished with exit code 0
